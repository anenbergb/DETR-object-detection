{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cbe6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f4d0709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9766814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embedding shape: torch.Size([3, 256, 224, 250])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def positional_encoding(x, y, num_pos_feats=128, temperature=10000):\n",
    "    \"\"\"\n",
    "    Generates sine-cosine positional embeddings.\n",
    "\n",
    "    It is important that the x-coordinates, and y-coordinates are normalized to [0, 1]\n",
    "    so that the model can learn relative positional relationships, regardless of the\n",
    "    input image's dimensions.\n",
    "\n",
    "    Args:\n",
    "        x: Normalized x-coordinates (tensor).\n",
    "        y: Normalized y-coordinates (tensor).\n",
    "        num_pos_feats: Number of positional features.\n",
    "        temperature: Temperature parameter for frequency scaling.\n",
    "\n",
    "    Returns:\n",
    "        Positional embeddings (tensor).\n",
    "    \"\"\"\n",
    "    scale = 2 * np.pi\n",
    "    x = x * scale\n",
    "    y = y * scale\n",
    "\n",
    "    # generate a set of exponentially spaced frequencies for the sine and cosine\n",
    "    # positional embeddings. These frequencies control the wavelengths of the sine\n",
    "    # and cosine waves, allowing the model to capture positional information\n",
    "    # at different scales.\n",
    "    # The frequencies step by 2 to ensure that the frequencies used for\n",
    "    # sine and cosines that are paired together are the same.\n",
    "    # e.g. [0,0,2,2,4,4,6,6,...,62,62]\n",
    "    dim_t = torch.arange(0, num_pos_feats, 2, dtype=torch.float32, device=x.device)\n",
    "    # dim_t / num_pos_feats normalizes the range to [0,1]\n",
    "    # {temperature**0 = 1, ...., temperature**1} exponential spacing of wavelengths\n",
    "    # from lower frequencies (longer wavelength) to higher frequencies\n",
    "    # (shorter wavelength)\n",
    "    dim_t = temperature ** (dim_t / num_pos_feats)\n",
    "\n",
    "    # Dividing the normalized coordinates by the frequency is equivalent to multipying by the period.\n",
    "    # We're scaling the coordinates based on the wavelength of the sine and cosine waves.\n",
    "    # We're converting the normalized coordinates [0, 2pi] into a phase value for the sine and cosine functions,\n",
    "    # which is what the sine and consine functions expect as their input.\n",
    "    # x[:,:,None] / dim_t broadcasting that each element in x is divided by each element of dim_t\n",
    "    # to create a new tensor of shape (batch_size, height, width, num_pos_feats)\n",
    "    # Each element pos_x[b, h, w, i] represents the normalized x-coordinate at position (h, w) in batch b,\n",
    "    # divided by the i-th frequency in dim_t.\n",
    "    pos_x = x[..., None] / dim_t\n",
    "    pos_y = y[..., None] / dim_t\n",
    "    # stack then flatten() will interleave the sine and cosine\n",
    "    pos_x = torch.stack((pos_x.sin(), pos_x.cos()), dim=-1).flatten(-2)\n",
    "    pos_y = torch.stack((pos_y.sin(), pos_y.cos()), dim=-1).flatten(-2)\n",
    "    pos = torch.cat((pos_y, pos_x), dim=-1).permute(0,3,1,2)\n",
    "    return pos\n",
    "\n",
    "def batch_positional_encoding(batch_shape, heights, widths, num_pos_feats=128, temperature=10000):\n",
    "    batch_grid_x = torch.zeros(batch_shape)\n",
    "    batch_grid_y = torch.zeros(batch_shape)\n",
    "    for batch_i, (height, width) in enumerate(zip(heights, widths)):\n",
    "\n",
    "        x_axis = torch.linspace(0, 1, width)\n",
    "        y_axis = torch.linspace(0, 1, height)\n",
    "        grid_y, grid_x = torch.meshgrid(y_axis, x_axis, indexing=\"ij\")\n",
    "        batch_grid_x[batch_i,:height,:width] = grid_x\n",
    "        batch_grid_y[batch_i,:height,:width] = grid_y\n",
    "    \n",
    "    return positional_encoding(batch_grid_x, batch_grid_y, num_pos_feats, temperature)\n",
    "\n",
    "# # Example usage:\n",
    "# batch_size, height, width = 2, 10, 10\n",
    "# x = torch.linspace(0, 1, width).unsqueeze(0).unsqueeze(0).repeat(batch_size, height, 1)\n",
    "# y = torch.linspace(0, 1, height).unsqueeze(0).unsqueeze(2).repeat(batch_size, 1, width)\n",
    "# pos_emb = positional_encoding(x, y, num_pos_feats=128)\n",
    "# print(f\"Positional embedding shape: {pos_emb.shape}\")\n",
    "\n",
    "\n",
    "batch_shape = (3, 224, 250) # B, H, W\n",
    "B,H,W = batch_shape\n",
    "heights = [168, 224, 200]\n",
    "widths = [168, 200, 250]\n",
    "pos_emb = batch_positional_encoding(batch_shape, heights, widths, 128)\n",
    "print(f\"Positional embedding shape: {pos_emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8f6c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should technically be able to do all the multiplications and stuff on 1D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56328fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a710d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shape = (3, 224, 256) # B, H, W\n",
    "B,H,W = batch_shape\n",
    "heights = [168, 224, 200]\n",
    "widths = [168, 200, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f251d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_axis = torch.arange(10)\n",
    "# y_axis = torch.arange(10)\n",
    "\n",
    "batch_grid_x = torch.zeros(batch_shape)\n",
    "batch_grid_y = torch.zeros(batch_shape)\n",
    "for batch_i, (height, width) in enumerate(zip(heights, widths)):\n",
    "    \n",
    "    x_axis = torch.linspace(0, 1, width)\n",
    "    y_axis = torch.linspace(0, 1, height)\n",
    "    grid_y, grid_x = torch.meshgrid(y_axis, x_axis, indexing=\"ij\")\n",
    "    batch_grid_x[batch_i,:height,:width] = grid_x\n",
    "    batch_grid_y[batch_i,:height,:width] = grid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81e45578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0060, 0.0120,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0060, 0.0120,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0060, 0.0120,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0050, 0.0101,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0050, 0.0101,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0050, 0.0101,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0050, 0.0101,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0050, 0.0101,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0050, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0039, 0.0078,  ..., 0.9922, 0.9961, 1.0000],\n",
       "         [0.0000, 0.0039, 0.0078,  ..., 0.9922, 0.9961, 1.0000],\n",
       "         [0.0000, 0.0039, 0.0078,  ..., 0.9922, 0.9961, 1.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_grid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01843c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_sin = torch.arange(4*3).reshape((1,2,2,3))\n",
    "# x_cos = torch.arange(4*3,2*4*3).reshape((1,2,2,3))\n",
    "# x_pos1 = torch.cat([x_sin, x_cos], dim=3)\n",
    "# x_pos2 = torch.stack([x_sin, x_cos], dim=4).flatten(3)\n",
    "# x_pos3 = torch.stack([x_sin, x_cos],dim=-1).flatten(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7bde912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detr.position_encoding import MyPositionalEncoding, PositionEmbeddingSine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b297199",
   "metadata": {},
   "outputs": [],
   "source": [
    "officialPosEncoding = PositionEmbeddingSine(num_pos_feats=128, normalize=True, eps=0)\n",
    "myPosEncoding = MyPositionalEncoding(num_pos_feats=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "151c0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shape = (3, 224, 256) # B, H, W\n",
    "B,H,W = batch_shape\n",
    "heights = [168, 224, 200]\n",
    "widths = [168, 200, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "92448ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones((batch_shape), dtype=torch.bool)\n",
    "for batch_index, (height, width) in enumerate(zip(heights, widths)):\n",
    "    mask[batch_index, :height, :width] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0cf09ace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "official_pos_embed = officialPosEncoding(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f735a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pos_embed = myPosEncoding(batch_shape, heights, widths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2ee215cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(official_pos_embed, my_pos_embed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-from-scratch]",
   "language": "python",
   "name": "conda-env-pytorch-from-scratch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
